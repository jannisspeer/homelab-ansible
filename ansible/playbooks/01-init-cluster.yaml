---
- name: Initialize Kubernetes control plane (first node)
  hosts: control_plane
  become: true
  tasks:
    - name: Check if cluster is already initialized
      stat:
        path: /etc/kubernetes/admin.conf
      register: kubeadm_initialized

    - name: Initialize Kubernetes cluster (first control plane node)
      shell: |
        kubeadm init \
          --pod-network-cidr={{ pod_network_cidr }} \
          --service-cidr={{ service_cidr }} \
          --apiserver-advertise-address={{ ansible_host }} \
          --control-plane-endpoint="{{ hostvars[groups['control_plane'][0]]['ansible_host'] }}:6443" \
          --upload-certs
      when: not kubeadm_initialized.stat.exists and inventory_hostname == groups['control_plane'][0]
      register: kubeadm_init
      failed_when: kubeadm_init.rc != 0

    - name: Create .kube directory for root
      file:
        path: /root/.kube
        state: directory
        mode: 0755
      when: inventory_hostname == groups['control_plane'][0]

    - name: Copy kubeconfig for root
      copy:
        src: /etc/kubernetes/admin.conf
        dest: /root/.kube/config
        remote_src: true
        owner: root
        group: root
        mode: 0600
      when: inventory_hostname == groups['control_plane'][0]

    - name: Create .kube directory for user
      file:
        path: /home/{{ ansible_user }}/.kube
        state: directory
        owner: "{{ ansible_user }}"
        group: "{{ ansible_user }}"
        mode: 0755
      when: inventory_hostname == groups['control_plane'][0]

    - name: Copy kubeconfig for user
      copy:
        src: /etc/kubernetes/admin.conf
        dest: /home/{{ ansible_user }}/.kube/config
        remote_src: true
        owner: "{{ ansible_user }}"
        group: "{{ ansible_user }}"
        mode: 0600
      when: inventory_hostname == groups['control_plane'][0]

    - name: Wait for kube-apiserver to be ready
      shell: kubectl get nodes
      become_user: "{{ ansible_user }}"
      environment:
        KUBECONFIG: /home/{{ ansible_user }}/.kube/config
      register: api_check
      until: api_check.rc == 0
      retries: 30
      delay: 2
      when: inventory_hostname == groups['control_plane'][0]

    - name: Ensure kubelet is running on first node
      systemd:
        name: kubelet
        state: started
        enabled: true
      when: inventory_hostname == groups['control_plane'][0]

    - name: Install Flannel CNI
      shell: kubectl apply -f https://github.com/flannel-io/flannel/releases/latest/download/kube-flannel.yml
      become_user: "{{ ansible_user }}"
      environment:
        KUBECONFIG: /home/{{ ansible_user }}/.kube/config
      when: inventory_hostname == groups['control_plane'][0]
      register: flannel_install

    - name: Wait for Flannel pods to be ready
      shell: kubectl wait --for=condition=Ready pod -l app=flannel -n kube-flannel --timeout=300s
      become_user: "{{ ansible_user }}"
      environment:
        KUBECONFIG: /home/{{ ansible_user }}/.kube/config
      when: inventory_hostname == groups['control_plane'][0]
      retries: 3
      delay: 10
      register: flannel_ready
      until: flannel_ready.rc == 0

    - name: Extract certificate key from init output
      set_fact:
        certificate_key: "{{ kubeadm_init.stdout | regex_search('--certificate-key ([a-f0-9]{64})', '\\1') | first }}"
      when: inventory_hostname == groups['control_plane'][0] and kubeadm_init.changed

    - name: Generate join command for control plane nodes
      shell: kubeadm token create --print-join-command --ttl 2h
      register: join_command_output
      when: inventory_hostname == groups['control_plane'][0]

    - name: Save control plane join command to file
      copy:
        content: "{{ join_command_output.stdout }} --control-plane --certificate-key {{ certificate_key }}"
        dest: /tmp/kubeadm_join_cp_command.sh
        mode: 0755
      when: inventory_hostname == groups['control_plane'][0] and certificate_key is defined

    - name: Fetch control plane join command to local
      fetch:
        src: /tmp/kubeadm_join_cp_command.sh
        dest: /tmp/kubeadm_join_cp_command.sh
        flat: true
      when: inventory_hostname == groups['control_plane'][0] and certificate_key is defined

    - name: Create etcd snapshot backup
      shell: |
        ETCDCTL_API=3 etcdctl \
          --endpoints=https://127.0.0.1:2379 \
          --cacert=/etc/kubernetes/pki/etcd/ca.crt \
          --cert=/etc/kubernetes/pki/etcd/server.crt \
          --key=/etc/kubernetes/pki/etcd/server.key \
          snapshot save /root/etcd-snapshot-$(date +%Y%m%d-%H%M%S).db
      when: inventory_hostname == groups['control_plane'][0]
      ignore_errors: true

    - name: Clean up join command on first node
      file:
        path: /tmp/kubeadm_join_cp_command.sh
        state: absent
      when: inventory_hostname == groups['control_plane'][0]

- name: Join additional control plane nodes
  hosts: control_plane
  become: true
  tasks:
    - name: Check if node is already joined
      stat:
        path: /etc/kubernetes/kubelet.conf
      register: kubelet_conf

    - name: Copy control plane join command to additional nodes
      copy:
        src: /tmp/kubeadm_join_cp_command.sh
        dest: /tmp/kubeadm_join_cp_command.sh
        mode: 0755
      when: not kubelet_conf.stat.exists and inventory_hostname != groups['control_plane'][0]

    - name: Join additional control plane nodes to cluster
      shell: /tmp/kubeadm_join_cp_command.sh
      when: not kubelet_conf.stat.exists and inventory_hostname != groups['control_plane'][0]
      register: join_result
      retries: 5
      delay: 10
      until: join_result.rc == 0

    - name: Ensure kubelet is running on joined nodes
      systemd:
        name: kubelet
        state: restarted
        enabled: true
      when: inventory_hostname != groups['control_plane'][0]

    - name: Create .kube directory for root on additional nodes
      file:
        path: /root/.kube
        state: directory
        mode: 0755
      when: inventory_hostname != groups['control_plane'][0]

    - name: Copy kubeconfig for root on additional nodes
      copy:
        src: /etc/kubernetes/admin.conf
        dest: /root/.kube/config
        remote_src: true
        owner: root
        group: root
        mode: 0600
      when: inventory_hostname != groups['control_plane'][0]

    - name: Create .kube directory for user on additional nodes
      file:
        path: /home/{{ ansible_user }}/.kube
        state: directory
        owner: "{{ ansible_user }}"
        group: "{{ ansible_user }}"
        mode: 0755
      when: inventory_hostname != groups['control_plane'][0]

    - name: Copy kubeconfig for user on additional nodes
      copy:
        src: /etc/kubernetes/admin.conf
        dest: /home/{{ ansible_user }}/.kube/config
        remote_src: true
        owner: "{{ ansible_user }}"
        group: "{{ ansible_user }}"
        mode: 0600
      when: inventory_hostname != groups['control_plane'][0]

    - name: Clean up join command
      file:
        path: /tmp/kubeadm_join_cp_command.sh
        state: absent
      when: inventory_hostname != groups['control_plane'][0]

- name: Remove control plane taints to allow workload scheduling
  hosts: control_plane
  tasks:
    - name: Wait for all nodes to be registered
      shell: kubectl get nodes --no-headers | wc -l
      register: node_count
      until: node_count.stdout | int == groups['control_plane'] | length
      retries: 30
      delay: 2
      become_user: "{{ ansible_user }}"
      environment:
        KUBECONFIG: /home/{{ ansible_user }}/.kube/config
      run_once: true

    - name: Remove NoSchedule taint from control plane nodes
      shell: |
        kubectl taint nodes {{ inventory_hostname }} node-role.kubernetes.io/control-plane:NoSchedule- || true
      become_user: "{{ ansible_user }}"
      environment:
        KUBECONFIG: /home/{{ ansible_user }}/.kube/config
      delegate_to: "{{ groups['control_plane'][0] }}"
      run_once: false

- name: Validate cluster health
  hosts: control_plane
  tasks:
    - name: Check all nodes are Ready
      shell: kubectl get nodes -o wide
      become_user: "{{ ansible_user }}"
      environment:
        KUBECONFIG: /home/{{ ansible_user }}/.kube/config
      register: cluster_nodes
      run_once: true

    - name: Display cluster status
      debug:
        var: cluster_nodes.stdout_lines
      run_once: true

    - name: Verify etcd cluster health
      shell: |
        ETCDCTL_API=3 etcdctl \
          --endpoints=https://127.0.0.1:2379 \
          --cacert=/etc/kubernetes/pki/etcd/ca.crt \
          --cert=/etc/kubernetes/pki/etcd/server.crt \
          --key=/etc/kubernetes/pki/etcd/server.key \
          endpoint health
      become: true
      register: etcd_health
      run_once: true
      ignore_errors: true

    - name: Display etcd health
      debug:
        var: etcd_health.stdout_lines
      run_once: true
      when: etcd_health.rc == 0

- name: Fetch kubeconfig to local machine
  hosts: control_plane
  tasks:
    - name: Fetch kubeconfig
      fetch:
        src: /etc/kubernetes/admin.conf
        dest: "{{ playbook_dir }}/../kubeconfig"
        flat: true
      become: true
      run_once: true